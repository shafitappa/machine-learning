{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using Keras VGG16 trained model (Transfer Learning)\n",
    "\n",
    "In this notebook, I am using keras Vgg16 model and appending a fully connected layer to vgg16 model and just training fully connected layer for classification.\n",
    "\n",
    "Steps followed are:\n",
    "\n",
    "1. Creatd a csv file using the text file from Flickr. This csv will have image_name, Logo_Name, Subset_class, x1,y1,x2,y2 co-ordinates of the images\n",
    "2. Using label encoder, added another column which contains numeric label for each logo\n",
    "3. Using the co-ordinates from the csv file, parsing the image using openCV\n",
    "4. Resize the parsed image and save the image to the drive\n",
    "5. Creating a new data frame which will have the file path, label other required information of the new image\n",
    "6. Using ImageDataGenerator from keras, Augumenting image by\n",
    "    i.   Rotating the image by 60 degrees\n",
    "    ii.  Changing the brightness of the image\n",
    "    iii. Flipping the image vertically and horizonatlly\n",
    "7. Created train, validation and test datasets and also respective ImageDataGenerators for each set\n",
    "8. Create object of Vgg16 trained model from keras and connected a fully connected network at the end of this vgg16 model\n",
    "9. Update the property of Vgg16 model so that it is not trained when training the classifier\n",
    "10. Define a check point which will save best weights, these weights are used to load the best model from training epochs\n",
    "11. Start training the model with defined number of epochs \n",
    "12. Here i am not using a custom optimizer. We can also use optimizer with our own hyper parameters but I am just making use \n",
    "    of Adam optimizer with default hyper parameters. If the accuracy of the model is not good, switch to optimizer with your       set of hyper parameters\n",
    "13. Plotting the train and validation loss which gives us a good understing of how model is fitting\n",
    "14. Load the best weights from training and compile the model. Save the model for future use\n",
    "15. Showing the accuracy of the model with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "#from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from scipy import ndimage\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Input\n",
    "from keras import applications, optimizers\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the csv using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Logo_Name</th>\n",
       "      <th>Subset_Class</th>\n",
       "      <th>X1</th>\n",
       "      <th>Y1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144503924.jpg</td>\n",
       "      <td>Adidas</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>234</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2451569770.jpg</td>\n",
       "      <td>Adidas</td>\n",
       "      <td>1</td>\n",
       "      <td>242</td>\n",
       "      <td>208</td>\n",
       "      <td>413</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>390321909.jpg</td>\n",
       "      <td>Adidas</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4761260517.jpg</td>\n",
       "      <td>Adidas</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>122</td>\n",
       "      <td>358</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4763210295.jpg</td>\n",
       "      <td>Adidas</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>63</td>\n",
       "      <td>130</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Image_Name Logo_Name  Subset_Class   X1   Y1   X2   Y2\n",
       "0   144503924.jpg    Adidas             1   38   12  234  142\n",
       "1  2451569770.jpg    Adidas             1  242  208  413  331\n",
       "2   390321909.jpg    Adidas             1   13    5   89   60\n",
       "3  4761260517.jpg    Adidas             1   43  122  358  354\n",
       "4  4763210295.jpg    Adidas             1   83   63  130   93"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converted the txt file from flicker to a csv \n",
    "image_data = pd.read_csv(\"Image_Labels.csv\")\n",
    "image_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using encoder to create numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(image_data.Logo_Name)\n",
    "image_data[\"Labels\"] = encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './flickr_logos_27_dataset_images'\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "data_dir = './Augmented_images'\n",
    "image_ext = \".jpg\"\n",
    "if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new dataframe by looping through each file from the training set. \n",
    "     Cropping the image using the given co-ordinates\n",
    "     Resizing the image to 32 X 32\n",
    "     Saving the image to local drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_image_df = pd.DataFrame(columns=[\"Image_File_Path\",\"Label\", \"Subset_label\", \"Brand_Name\"])\n",
    "for index, row in image_data.iterrows():\n",
    "    file_path = os.path.join(image_path, row[\"Image_Name\"])\n",
    "    label =row[\"Labels\"] \n",
    "    subset_class =row[\"Subset_Class\"]\n",
    "    brand_name = row[\"Logo_Name\"]\n",
    "    #print(file_path)\n",
    "    image = cv2.imread(file_path)\n",
    "    y1,y2,x1,x2 = int(row[\"Y1\"]),int(row[\"Y2\"]),int(row[\"X1\"]),int(row[\"X2\"])\n",
    "    crop_img = image[y1:y2, x1:x2]\n",
    "    # Shafi : For few images, co ordinates are not proper. For example 2662264721.jpg has co-ordinates\n",
    "    #         as 3,197,3,197 which would result in a image of size zero or an empty image            \n",
    "    if crop_img.size > 0:\n",
    "        resized_img = cv2.resize(crop_img, (image_width,image_height))\n",
    "        new_file_name = str(index)+\"_\"+brand_name+\"_\"+str(subset_class)+\"_\"+str(label)+image_ext\n",
    "        newfile_path = os.path.join(data_dir,new_file_name)\n",
    "        augmented_image_df.loc[len(augmented_image_df)] = [newfile_path,label, subset_class,brand_name]\n",
    "        cv2.imwrite(newfile_path,resized_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing custom image augmentation on the cropped logo dataset by using ImageDataGenerator\n",
    "\n",
    "1. Rotating the image\n",
    "2. Flipping the image\n",
    "3. Changing brightness of the image\n",
    "4. Also required image pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the data set is already cropped, i am not using vertical and horizontal shift\n",
    "# genrating new set of data by rotating, flipping and changing the brightness of the image\n",
    "datagen = ImageDataGenerator(   featurewise_center=True,\n",
    "                                featurewise_std_normalization=True,\n",
    "                                rotation_range=60,\n",
    "                                brightness_range=[0.3,0.7],\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip = True)\n",
    "\n",
    "valgen = ImageDataGenerator(   featurewise_center=True,\n",
    "                                featurewise_std_normalization=True,\n",
    "                                rotation_range=60,\n",
    "                                brightness_range=[0.3,0.7],\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip = True)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train, test and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_frame, test_data_frame = train_test_split(augmented_image_df, test_size=0.2)\n",
    "train_df, validation_df = train_test_split(train_data_frame, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating ImageDataGenerators for train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3624 images belonging to 27 classes.\n",
      "Found 725 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_dataframe(train_data_frame,x_col = 'Image_File_Path',\n",
    "y_col = 'Brand_Name',\n",
    "directory = None,\n",
    "target_size=(image_width, image_height),\n",
    "batch_size=batch_size,\n",
    "class_mode='sparse')\n",
    "\n",
    "\n",
    "validation_generator = valgen.flow_from_dataframe(validation_df,x_col = 'Image_File_Path',\n",
    "y_col = 'Brand_Name',\n",
    "directory = None,\n",
    "target_size=(image_width, image_height),\n",
    "batch_size=batch_size,\n",
    "class_mode='sparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating object of VGG16 from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37_64bit\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(image_width, image_height, 3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing early layers of VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a sequntial model and adding VGG16 as the first layer and also adding other dense alyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37_64bit\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                3483      \n",
      "=================================================================\n",
      "Total params: 14,882,395\n",
      "Trainable params: 167,707\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tl = Sequential()\n",
    "model_tl.add(base_model)\n",
    "model_tl.add(Flatten())\n",
    "\n",
    "model_tl.add(Dense(256, activation=\"relu\"))\n",
    "model_tl.add(Dropout(0.5))\n",
    "model_tl.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "model_tl.add(Dense(27, activation=\"softmax\"))\n",
    "model_tl.summary()\n",
    "checkpoint_name = './Transfer_learning_weights.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a check point to save the best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling\n",
    "model_tl.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\", metrics=['accuracy'])\n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_acc', verbose=0, save_best_only=True, mode='auto',\n",
    "                                 save_weights_only=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37_64bit\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37_64bit\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "c:\\python37_64bit\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:707: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 20s 141ms/step - loss: 6.3898 - acc: 0.2951 - val_loss: 1.2412 - val_acc: 0.6792\n",
      "Epoch 2/25\n",
      "144/144 [==============================] - 20s 138ms/step - loss: 1.9302 - acc: 0.5733 - val_loss: 0.7406 - val_acc: 0.8000\n",
      "Epoch 3/25\n",
      "144/144 [==============================] - 20s 139ms/step - loss: 1.3495 - acc: 0.6701 - val_loss: 0.5448 - val_acc: 0.8468\n",
      "Epoch 4/25\n",
      "144/144 [==============================] - 20s 138ms/step - loss: 1.0810 - acc: 0.7174 - val_loss: 0.4264 - val_acc: 0.8794\n",
      "Epoch 5/25\n",
      "144/144 [==============================] - 20s 141ms/step - loss: 0.9281 - acc: 0.7566 - val_loss: 0.3192 - val_acc: 0.9149\n",
      "Epoch 6/25\n",
      "144/144 [==============================] - 21s 147ms/step - loss: 0.7741 - acc: 0.7913 - val_loss: 0.2778 - val_acc: 0.9277\n",
      "Epoch 7/25\n",
      "144/144 [==============================] - 21s 143ms/step - loss: 0.6870 - acc: 0.8167 - val_loss: 0.3111 - val_acc: 0.9121\n",
      "Epoch 8/25\n",
      "144/144 [==============================] - 21s 145ms/step - loss: 0.6609 - acc: 0.8146 - val_loss: 0.2656 - val_acc: 0.9234\n",
      "Epoch 9/25\n",
      "144/144 [==============================] - 20s 140ms/step - loss: 0.6682 - acc: 0.8295 - val_loss: 0.2488 - val_acc: 0.9305\n",
      "Epoch 10/25\n",
      "144/144 [==============================] - 20s 139ms/step - loss: 0.6131 - acc: 0.8344 - val_loss: 0.2228 - val_acc: 0.9404\n",
      "Epoch 11/25\n",
      "144/144 [==============================] - 20s 139ms/step - loss: 0.5242 - acc: 0.8500 - val_loss: 0.1838 - val_acc: 0.9546\n",
      "Epoch 12/25\n",
      "144/144 [==============================] - 20s 139ms/step - loss: 0.5005 - acc: 0.8663 - val_loss: 0.1845 - val_acc: 0.9461\n",
      "Epoch 13/25\n",
      "144/144 [==============================] - 20s 137ms/step - loss: 0.5655 - acc: 0.8493 - val_loss: 0.1352 - val_acc: 0.9645\n",
      "Epoch 14/25\n",
      "144/144 [==============================] - 20s 138ms/step - loss: 0.5213 - acc: 0.8639 - val_loss: 0.1586 - val_acc: 0.9589\n",
      "Epoch 15/25\n",
      "144/144 [==============================] - 20s 138ms/step - loss: 0.4683 - acc: 0.8715 - val_loss: 0.1007 - val_acc: 0.9688\n",
      "Epoch 16/25\n",
      "144/144 [==============================] - 20s 139ms/step - loss: 0.4288 - acc: 0.8816 - val_loss: 0.1041 - val_acc: 0.9787\n",
      "Epoch 17/25\n",
      "144/144 [==============================] - 20s 137ms/step - loss: 0.4183 - acc: 0.8823 - val_loss: 0.0895 - val_acc: 0.9716\n",
      "Epoch 18/25\n",
      "144/144 [==============================] - 20s 137ms/step - loss: 0.4037 - acc: 0.8875 - val_loss: 0.1112 - val_acc: 0.9660\n",
      "Epoch 19/25\n",
      "144/144 [==============================] - 20s 139ms/step - loss: 0.3898 - acc: 0.8951 - val_loss: 0.1106 - val_acc: 0.9702\n",
      "Epoch 20/25\n",
      "144/144 [==============================] - 20s 138ms/step - loss: 0.3947 - acc: 0.8944 - val_loss: 0.0830 - val_acc: 0.9830\n",
      "Epoch 21/25\n",
      "144/144 [==============================] - 20s 138ms/step - loss: 0.3772 - acc: 0.8920 - val_loss: 0.0842 - val_acc: 0.9801\n",
      "Epoch 22/25\n",
      "144/144 [==============================] - 20s 137ms/step - loss: 0.3759 - acc: 0.8972 - val_loss: 0.0688 - val_acc: 0.9816\n",
      "Epoch 23/25\n",
      "144/144 [==============================] - 20s 137ms/step - loss: 0.4098 - acc: 0.8969 - val_loss: 0.1020 - val_acc: 0.9688\n",
      "Epoch 24/25\n",
      "144/144 [==============================] - 20s 138ms/step - loss: 0.3298 - acc: 0.9080 - val_loss: 0.0591 - val_acc: 0.9801\n",
      "Epoch 25/25\n",
      "144/144 [==============================] - 20s 142ms/step - loss: 0.2918 - acc: 0.9181 - val_loss: 0.0716 - val_acc: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2225655dc18>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tl.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_df)//batch_size,\n",
    "        epochs=25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_df)//batch_size, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Trainning and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation loss\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss = model_tl.history.history['loss']\n",
    "valid_loss = model_tl.history.history['val_loss']\n",
    "epochs = len(train_loss)\n",
    "print(\"Training and Validation loss\")\n",
    "x_axis = range(0, epochs)\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(x_axis, train_loss, label='Train')\n",
    "plt.plot(x_axis, valid_loss, label='Validation')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoc\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the best weights from the training and compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl.load_weights('Transfer_learning_weights.hdf5')\n",
    "model_tl.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl.save(\"Transfer_learning_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating test ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 907 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "testgen = ImageDataGenerator(   featurewise_center=True,\n",
    "                                featurewise_std_normalization=True,\n",
    "                                rotation_range=30,\n",
    "                                brightness_range=[0.25,0.75],\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip = True)\n",
    "\n",
    "\n",
    "test_generator = testgen.flow_from_dataframe(test_data_frame,x_col = 'Image_File_Path',\n",
    "y_col = 'Brand_Name',\n",
    "directory = None,\n",
    "target_size=(image_width, image_height),\n",
    "batch_size=batch_size,\n",
    "class_mode='sparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the model with test data and printing the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37_64bit\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "c:\\python37_64bit\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:707: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is :  94.81808087791602 %\n"
     ]
    }
   ],
   "source": [
    "nb_samples = len(test_data_frame)\n",
    "\n",
    "test_score = model_tl.evaluate_generator(test_generator,steps = nb_samples/batch_size)\n",
    "\n",
    "print(\"Test accuracy is : \", test_score[1]*100, \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
